{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2JoKEMnqVzzu"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM7fDDQxa3sBHDHEbyjrHg/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbrahamGarcia240/DrakarGPT/blob/main/Drakar_GPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python imports\n",
        "\n",
        "# 3rd party imports\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "ANy-SKa5G8zp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"The device we will use is '{DEVICE}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqfRNCy8PYG7",
        "outputId": "0fb8d507-0efd-4de5-83fd-0cdf82ba4cd8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The device we will use is 'cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset to use\n",
        "In this case I want to recreate George R. R. Martin writing style"
      ],
      "metadata": {
        "id": "s_LTXx--CE6N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPT6NtjJBzEu",
        "outputId": "ceaaffd8-d00d-4475-831e-38901ff863a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-09-06 03:04:25--  https://raw.githubusercontent.com/nihitx/game-of-thrones-/master/gameofthrones.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5826890 (5.6M) [text/plain]\n",
            "Saving to: ‘gameofthrones.txt’\n",
            "\n",
            "gameofthrones.txt   100%[===================>]   5.56M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-09-06 03:04:26 (198 MB/s) - ‘gameofthrones.txt’ saved [5826890/5826890]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/nihitx/game-of-thrones-/master/gameofthrones.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a tokenizer and de-tokenizer"
      ],
      "metadata": {
        "id": "l9gH5xiwCrOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the content of the file\n",
        "DATASET_FILE = \"gameofthrones.txt\"\n",
        "with open(DATASET_FILE, encoding=\"utf-8\") as f:\n",
        "    DATASET = f.read()\n",
        "print(DATASET[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVxsBqA0CkXO",
        "outputId": "52226a03-8f0c-42a0-d2b9-64522894bc21"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "“We should start back,” Gared urged as the woods began to grow dark around them. “The wildlings ar\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all the tokens in the vocabulary, in this case\n",
        "# I will tokenize on the character level, i.e.\n",
        "# John Snow -> ['J', 'o', 'h', 'n', ' ', 'S', 'n', 'o', 'w']\n",
        "VOCABULARY = list(set(DATASET))\n",
        "print(\"The vocabulary tokens are:\")\n",
        "print(''.join(VOCABULARY))\n",
        "print(f\"The number of tokens in our vocabulary is '{len(VOCABULARY)}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpCC5zWEDDTI",
        "outputId": "9286ab31-7078-45f6-e9c1-fe8f25e404ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary tokens are:\n",
            "VN’KY“Qv8TiOpn;]Z.{hy7!eoS/gw90 })LrE3Hz-ltfI6su…m,ê”G‘b\n",
            "dJjCMé:x2U4R(PB1qakWXcD[?—A5F\n",
            "The number of tokens in our vocabulary is '86'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# I think it would make sense to try reducing the vocabulary size by getting rid\n",
        "# of uppercase and lowercase strings\n",
        "DATASET = DATASET.lower()\n",
        "VOCABULARY = list(set(DATASET))\n",
        "print(\"The vocabulary tokens are:\")\n",
        "print(''.join(VOCABULARY))\n",
        "VOCAB_SIZE = len(VOCABULARY)\n",
        "print(f\"The number of tokens in our vocabulary is '{len(VOCABULARY)}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eVxfReHeEREy",
        "outputId": "689ad544-2741-4148-a722-74ed64bfdae0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary tokens are:\n",
            "74!z-(’el3otf6“suq…1vam8k,/ipênw”g;90]‘ b\n",
            "cd[}?j—.{)éhr5y:x2\n",
            "The number of tokens in our vocabulary is '60'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a mapping from word to index and index to word\n",
        "word_to_idx = {word: idx for idx, word in enumerate(VOCABULARY)}\n",
        "idx_to_word = {idx: word for idx, word in enumerate(VOCABULARY)}\n",
        "\n",
        "# Create a quick function to tokenize\n",
        "def tokenize(word: str):\n",
        "  return [word_to_idx[token] for token in word]\n",
        "\n",
        "def detokenize(tokens: list):\n",
        "  return \"\".join([idx_to_word[idx] for idx in tokens])\n",
        "\n",
        "# Let's try an example\n",
        "sentence = \"john snow\"\n",
        "print(f\"The sentence I will tokenize is '{sentence}\")\n",
        "print(f\"The tokenized sentence is '{tokenize(sentence)}'\")\n",
        "print(f\"The detokenized sentence is '{detokenize(tokenize(sentence))}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuKQTtATFkNS",
        "outputId": "f29a1ea2-e7e7-4808-b18f-87fbee1f3d43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentence I will tokenize is 'john snow\n",
            "The tokenized sentence is '[47, 10, 53, 30, 39, 15, 30, 10, 31]'\n",
            "The detokenized sentence is 'john snow'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the whole dataset\n",
        "tokenized_dataset = torch.tensor(tokenize(DATASET), dtype=torch.long)\n",
        "print(f\"The tokenized dataset is of length '{len(tokenized_dataset)}'\")\n",
        "print(tokenized_dataset[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqNrSYpHHOlJ",
        "outputId": "d78d1214-671c-49b7-9d97-e0a1c1e81918"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokenized dataset is of length '5662324'\n",
            "tensor([41, 41, 14, 31,  7, 39, 15, 53, 10, 16,  8, 43, 39, 15, 11, 21, 54, 11,\n",
            "        39, 40, 21, 42, 24, 25, 32, 39, 33, 21, 54,  7, 43, 39, 16, 54, 33,  7,\n",
            "        43, 39, 21, 15, 39, 11, 53,  7, 39, 31, 10, 10, 43, 15, 39, 40,  7, 33,\n",
            "        21, 30, 39, 11, 10, 39, 33, 54, 10, 31, 39, 43, 21, 54, 24, 39, 21, 54,\n",
            "        10, 16, 30, 43, 39, 11, 53,  7, 22, 49, 39, 14, 11, 53,  7, 39, 31, 27,\n",
            "         8, 43,  8, 27, 30, 33, 15, 39, 21, 54,  7, 39, 43,  7, 21, 43, 49, 32,\n",
            "        41, 41, 14, 43, 10, 39, 11, 53,  7, 39, 43,  7, 21, 43, 39, 12, 54, 27,\n",
            "        33, 53, 11,  7, 30, 39, 56, 10, 16, 46, 32, 39, 15,  7, 54, 39, 31, 21,\n",
            "        56, 22, 21, 54, 39, 54, 10, 56, 42,  7, 39, 21, 15, 24,  7, 43, 39, 31,\n",
            "        27, 11, 53, 39, 47, 16, 15, 11, 39, 11, 53,  7, 39, 53, 27, 30, 11, 39,\n",
            "        10, 12, 39, 21, 39, 15, 22, 27,  8,  7, 49, 41, 41, 33, 21, 54,  7, 43,\n",
            "        39, 43, 27, 43, 39, 30, 10, 11, 39, 54, 27, 15,  7, 39, 11, 10, 39, 11,\n",
            "        53,  7, 39, 40, 21, 27, 11, 49, 39, 53,  7, 39, 31, 21, 15, 39, 21, 30,\n",
            "        39, 10,  8, 43, 39, 22, 21, 30, 25, 39, 28, 21, 15, 11, 39, 12, 27, 12,\n",
            "        11, 56, 25, 39, 21, 30, 43, 39, 53,  7, 39, 53, 21, 43, 39, 15,  7,  7,\n",
            "        30, 39, 11, 53,  7, 39,  8, 10, 54, 43,  8, 27, 30, 33, 15, 39, 42, 10,\n",
            "        22,  7, 39, 21, 30, 43, 39, 33, 10, 49, 39, 14, 43,  7, 21, 43, 39, 27,\n",
            "        15, 39, 43,  7, 21, 43, 25, 32, 39, 53,  7, 39, 15, 21, 27, 43, 49, 39,\n",
            "        14, 31,  7, 39, 53, 21, 20,  7, 39, 30, 10, 39, 40, 16, 15, 27, 30,  7,\n",
            "        15, 15, 39, 31, 27, 11, 53, 39, 11, 53,  7, 39, 43,  7, 21, 43, 49, 32,\n",
            "        41, 41, 14, 21, 54,  7, 39, 11, 53,  7, 56, 39, 43,  7, 21, 43, 46, 32,\n",
            "        39, 54, 10, 56, 42,  7, 39, 21, 15, 24,  7, 43, 39, 15, 10, 12, 11,  8,\n",
            "        56, 49, 39, 14, 31, 53, 21, 11, 39, 28, 54, 10, 10, 12, 39, 53, 21, 20,\n",
            "         7, 39, 31,  7, 46, 32, 41, 41, 14, 31, 27,  8,  8, 39, 15, 21, 31, 39,\n",
            "        11, 53,  7, 22, 25, 32, 39, 33, 21, 54,  7, 43, 39, 15, 21, 27, 43, 49,\n",
            "        39, 14, 27, 12, 39, 53,  7, 39, 15, 21, 56, 15, 39, 11, 53,  7, 56, 39,\n",
            "        21, 54,  7, 39, 43,  7, 21, 43, 25, 39, 11, 53, 21, 11,  6, 15, 39, 28,\n",
            "        54, 10, 10, 12, 39,  7, 30, 10, 16, 33, 53, 39, 12, 10, 54, 39, 22,  7,\n",
            "        49, 32, 41, 41, 31, 27,  8,  8, 39, 53, 21, 43, 39, 24, 30, 10, 31, 30,\n",
            "        39, 11, 53,  7, 56, 39, 31, 10, 16,  8, 43, 39, 43, 54, 21, 33, 39, 53,\n",
            "        27, 22, 39, 27, 30, 11, 10, 39, 11, 53,  7, 39, 17, 16, 21, 54, 54,  7,\n",
            "         8, 39, 15, 10, 10, 30,  7, 54, 39, 10, 54, 39,  8, 21, 11,  7, 54, 49,\n",
            "        39, 53,  7, 39, 31, 27, 15, 53,  7, 43, 39, 27, 11, 39, 53, 21, 43, 39,\n",
            "        40,  7,  7, 30, 39,  8, 21, 11,  7, 54, 39, 54, 21, 11, 53,  7, 54, 39,\n",
            "        11, 53, 21, 30, 39, 15, 10, 10, 30,  7, 54, 49, 39, 14, 22, 56, 39, 22,\n",
            "        10, 11, 53,  7, 54, 39, 11, 10,  8, 43, 39, 22,  7, 39, 11, 53, 21, 11,\n",
            "        39, 43,  7, 21, 43, 39, 22,  7, 30, 39, 15, 27, 30, 33, 39, 30, 10, 39,\n",
            "        15, 10, 30, 33, 15, 25, 32, 39, 53,  7, 39, 28, 16, 11, 39, 27, 30, 49,\n",
            "        41, 41, 14, 22, 56, 39, 31,  7, 11, 39, 30, 16, 54, 15,  7, 39, 15, 21,\n",
            "        27, 43, 39, 11, 53,  7, 39, 15, 21, 22,  7, 39, 11, 53, 27, 30, 33, 25,\n",
            "        39, 31, 27,  8,  8, 25, 32, 39, 54, 10, 56, 42,  7, 39, 54,  7, 28,  8,\n",
            "        27,  7, 43, 49, 39, 14, 30,  7, 20,  7, 54, 39, 40,  7,  8, 27,  7, 20,\n",
            "         7, 39, 21, 30, 56, 11, 53, 27, 30, 33, 39, 56, 10, 16, 39, 53,  7, 21,\n",
            "        54, 39, 21, 11, 39, 21, 39, 31, 10, 22, 21, 30,  6, 15, 39, 11, 27, 11,\n",
            "        49, 39, 11, 53,  7, 54,  7, 39, 21, 54,  7, 39, 11, 53, 27, 30, 33, 15,\n",
            "        39, 11, 10, 39, 40,  7, 39,  8,  7, 21, 54, 30,  7, 43, 39,  7, 20,  7,\n",
            "        30, 39, 12, 54, 10, 22, 39, 11, 53,  7, 39, 43,  7, 21, 43, 49, 32, 39,\n",
            "        53, 27, 15, 39, 20, 10, 27, 42,  7, 39,  7, 42, 53, 10,  7, 43, 25, 39,\n",
            "        11, 10, 10, 39,  8, 10, 16, 43, 39, 27, 30, 39, 11, 53,  7, 39, 11, 31,\n",
            "        27,  8, 27, 11, 39, 12, 10, 54,  7, 15, 11, 49, 41, 41, 14, 31,  7, 39,\n",
            "        53, 21, 20,  7, 39, 21, 39,  8, 10, 30, 33, 39, 54, 27, 43,  7, 39, 40,\n",
            "         7, 12, 10, 54,  7, 39, 16, 15, 25, 32, 39, 33, 21, 54,  7, 43, 39, 28,\n",
            "        10, 27, 30, 11,  7, 43, 39, 10, 16, 11, 49, 39, 14,  7, 27, 33, 53, 11,\n",
            "        39, 43, 21, 56, 15, 25, 39, 22, 21, 56, 40,  7, 39, 30, 27, 30,  7, 49,\n",
            "        39, 21, 30, 43, 39, 30, 27, 33, 53, 11, 39, 27, 15, 39, 12, 21,  8,  8,\n",
            "        27, 30, 33, 49, 32, 41, 41, 15,  7, 54])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a train and test dataset\n"
      ],
      "metadata": {
        "id": "tT0sAyC7GzCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the first 80% of the words to be the training set\n",
        "training_size = int(0.8 * len(tokenized_dataset))\n",
        "train_data = tokenized_dataset[:training_size]\n",
        "test_data = tokenized_dataset[training_size:]"
      ],
      "metadata": {
        "id": "Ucfj5qGtGzxu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a block size for the chunks that we will use to train the transformer\n",
        "# if the block size is 8, it means that we will use AT MOST 8 characters as\n",
        "# context, i.e.\n",
        "#\n",
        "#     Chunk = [51, 45, 41,  1,  1, 16, 24, 40] which is extracted from\n",
        "#             [51, 45, 41,  1,  1, 16, 24, 40, 1]\n",
        "#\n",
        "# That means that we can get any of these entries:\n",
        "#\n",
        "#  context [51], we expect [45]\n",
        "#  context [51, 45], we expect [41]\n",
        "#. context [51, 45, 41] we expect [1]\n",
        "#. context [51, 45, 41,  1] we expect [1]\n",
        "#. context [51, 45, 41,  1,  1] we expect [16]\n",
        "#. context [51, 45, 41,  1,  1, 16] we expect [24]\n",
        "#  context [51, 45, 41,  1,  1, 16, 24] we expect [40]\n",
        "#. context [51, 45, 41,  1,  1, 16, 24, 40] we expect [1]\n",
        "#\n",
        "# This way we can train the transformer to be able to generate text with\n",
        "# as little as only one token as context\n",
        "BLOCK_SIZE = 256\n"
      ],
      "metadata": {
        "id": "t6cZJp8bPvAe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to generate batches of data, each batch will have\n",
        "# B elements of size BLOCK_SIZE\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "def get_batch(dataset_type=\"train\") -> tuple[torch.tensor, torch.tensor]:\n",
        "  if dataset_type == \"train\":\n",
        "    data = train_data\n",
        "  elif dataset_type == \"test\":\n",
        "    data = test_data\n",
        "\n",
        "  # Get a random index from the dataset we are working with\n",
        "  # Whatever index we get we need to ensure we will have AT LEAST\n",
        "  # BLOCK_SIZE indexes in front of it\n",
        "  #\n",
        "  # We need to specify how many indexes we want to get, in this case\n",
        "  # we want BATCH_SIZE\n",
        "  rand_idx = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "\n",
        "  # Now at this point we have BATCH_SIZE indexes so we need to get the\n",
        "  # actual sequence out of the indexes\n",
        "  x = torch.stack([data[idx:idx + BLOCK_SIZE] for idx in rand_idx])\n",
        "  y = torch.stack([data[idx + 1:idx + BLOCK_SIZE + 1] for idx in rand_idx])\n",
        "\n",
        "  # Return both X and Y for the batch, if we have 4 elements in the batch, then\n",
        "  # X will have 4 vectors size BLOCK_SIZE and Y will have 4 vectors size\n",
        "  # BLOCK_SIZE, however, Y will be one index ahead from X\n",
        "\n",
        "  # Ensure to move the data to the device\n",
        "  x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "  return x, y\n",
        "\n",
        "\n",
        "x, y = get_batch(\"train\")\n",
        "print(f\"The shape of x is '{x.shape}'\")\n",
        "print(x)\n",
        "print(f\"The shape of y is '{y.shape}'\")\n",
        "print(y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQrrw5kEcE7L",
        "outputId": "30c571a2-9a3b-493b-f0bc-b3a47e4690e4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of x is 'torch.Size([64, 256])'\n",
            "tensor([[39, 42, 10,  ...,  7, 54, 15],\n",
            "        [56, 39, 22,  ..., 39, 15, 53],\n",
            "        [21,  8, 56,  ..., 40, 10, 56],\n",
            "        ...,\n",
            "        [39, 10, 54,  ..., 54, 10,  8],\n",
            "        [42, 27, 43,  ..., 39, 21, 39],\n",
            "        [30, 43, 39,  ..., 54, 39, 10]], device='cuda:0')\n",
            "The shape of y is 'torch.Size([64, 256])'\n",
            "tensor([[42, 10, 30,  ..., 54, 15, 25],\n",
            "        [39, 22, 10,  ..., 15, 53, 10],\n",
            "        [ 8, 56, 30,  ..., 10, 56, 39],\n",
            "        ...,\n",
            "        [10, 54, 39,  ..., 10,  8,  7],\n",
            "        [27, 43,  7,  ..., 21, 39, 15],\n",
            "        [43, 39, 10,  ..., 39, 10, 12]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a seed\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Create a class for the Bigram Model\n",
        "class BigramModel(nn.Module):\n",
        "  def __init__(self, vocab_size: int):\n",
        "    super().__init__()\n",
        "\n",
        "    # I want my embeddings to have the same size of the vocabulary, this is\n",
        "    # because the vocabulary size is not too big, normally we would like\n",
        "    # a smaller size for the embedding layer\n",
        "    self.embedding_layer = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "  def forward(self, x, y = None):\n",
        "    loss = None\n",
        "    # Feed the tokens to the embedding layer\n",
        "    # If the input is (Batch_size, Block_size), the output will be\n",
        "    # (Batch_size, Block_size, Vocab_size) since for each token in the input\n",
        "    # we are creating an embedding\n",
        "    y_hat = self.embedding_layer(x)\n",
        "\n",
        "    # Compute the loss of the predictions\n",
        "    # y_hat is (Batch_size, Block_size, Vocab_size), and\n",
        "    # y is (Batch_size, Block_size)\n",
        "    #\n",
        "    # In multiclass classification normally the y's are the classes, either\n",
        "    # [1, 2, 9, 2, ... etc]\n",
        "    # And the y_hats are the logits, ie, for each entry in y there is a vector\n",
        "    # of size len(classes) where index of the element with the biggest value is\n",
        "    #. expecting to match y[idx], example:\n",
        "    #\n",
        "    # Suppose we have 5 classes\n",
        "    # y[0] = 1\n",
        "    # y_hat[0] = [0, 0.8, 0.05, 0.05, 0]\n",
        "    #\n",
        "    # This means that we need y to be an array of the expected classes rather\n",
        "    # than a matrix i.e.\n",
        "    #\n",
        "    #.   y = [[34,  1, 53, 35],\n",
        "    #         [0, 56, 17, 39]]\n",
        "    #\n",
        "    # We really want y to be the whole sequence:\n",
        "    #.   y = [34,  1, 53, 35, 0, 56, 17, 39]\n",
        "    if y is not None:\n",
        "      y = y.view(BATCH_SIZE * BLOCK_SIZE)\n",
        "    # Similarly since the NN is trying to predict EVERY word in the sequence\n",
        "    # and instead of tokens we get embeddings, then we want y_hat to be a matrix\n",
        "    # of size (batch_size * block_size, vocab_size), i.e\n",
        "    #\n",
        "    #.  Assume that 'a', 'b', ...'z' are embeddings predicted by the NN of size\n",
        "    #.  vocab_size each... i.e. 'a' = [003, 53465, 5352342, ..., 4354]\n",
        "    #\n",
        "    #.   y_hat = [[a, b, c, d],\n",
        "    #             [b, k, d, c]]\n",
        "    #\n",
        "    #.   y_hat = [a, b, c, d, b, k, d, c]\n",
        "    #\n",
        "    # In this way, similarly to the inicial example where binary cross entropy\n",
        "    # compared an scalar to a vector, we can do:\n",
        "    #\n",
        "    #       y = [34, 1, 53, 35, 0, 56, 17, 39]\n",
        "    #                        vs\n",
        "    #   y_hat = [a,  b,  c,  d, b,  k,  d,  c]\n",
        "    #\n",
        "    #. And hope that 'a' should be the embedding for 34\n",
        "    if y is not None:\n",
        "      batch_size, block_size, vocab_size = y_hat.shape\n",
        "      y_hat = y_hat.view(batch_size * block_size, vocab_size)\n",
        "      loss = F.cross_entropy(y_hat, y)\n",
        "\n",
        "      # Restore y_hat to return a consistent size\n",
        "      y_hat = y_hat.view(batch_size, block_size, vocab_size)\n",
        "    return y_hat, loss\n",
        "\n",
        "  def generate(self, x, max_new_tokens):\n",
        "    # x has the shape (BATCH_SIZE, BLOCK_SIZE)\n",
        "    for _ in range(max_new_tokens):\n",
        "      # Get the predictions\n",
        "      # y_hat is shaped (BATCH_SIZE, BLOCK_SIZE, vocab_size) because we did not\n",
        "      # provided a true labe to the forward function\n",
        "      y_hat, _ = self(x)\n",
        "\n",
        "      # Get only the last generated embedding for each element in the batch\n",
        "      # This will become a (BATCH_SIZE, vocab_size)\n",
        "      last_embeddings = y_hat[:, -1, :]\n",
        "      # Run it over softmax\n",
        "      # (BATCH_SIZE, vocab_size)\n",
        "      last_embeddings_scores = F.softmax(last_embeddings, dim=-1)\n",
        "      # Get the index of the biggest score\n",
        "      # (BATCH_SIZE, 1)\n",
        "      high_score_indexes = torch.multinomial(last_embeddings_scores,\n",
        "                                             num_samples=1)\n",
        "      # Concatenate the predicted scores to the input for the next generation\n",
        "      # (BATCH_SIZE, BLOCK_SIZE + 1)\n",
        "      x = torch.cat((x, high_score_indexes), dim=1)\n",
        "    return x\n",
        "\n",
        "# Run an example\n",
        "print(f\"The vocabulary size is {len(VOCABULARY)}\")\n",
        "model = BigramModel(len(VOCABULARY))\n",
        "# Move the model to the device\n",
        "model.to(DEVICE)\n",
        "\n",
        "x, y = get_batch(\"train\")\n",
        "print(f\"The shape of x is '{x.shape}'\")\n",
        "print(f\"The shape of y is '{y.shape}'\")\n",
        "output, loss = model(x, y)\n",
        "print(f\"The shape of output is '{output.shape}'\")\n",
        "print(f\"The loss of the model is '{loss}'\")\n",
        "\n",
        "# We can start generating out of a space\n",
        "input = \" \"\n",
        "tokenized_input = torch.tensor(tokenize(input), dtype=torch.long)\n",
        "# We will do no batching\n",
        "# Input shape will be (1, 1)\n",
        "tokenized_input = tokenized_input.view(1, 1).to(DEVICE)\n",
        "generated_tokens = model.generate(tokenized_input, 100)[0].tolist()\n",
        "generated_text = detokenize(generated_tokens)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrhdWZ78gijj",
        "outputId": "7c0dcdbc-21a2-44a1-95ac-6588d762e7ee"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary size is 60\n",
            "The shape of x is 'torch.Size([64, 256])'\n",
            "The shape of y is 'torch.Size([64, 256])'\n",
            "The shape of output is 'torch.Size([64, 256, 60])'\n",
            "The loss of the model is '4.684937477111816'\n",
            " (d(,57lsw“z’\n",
            "y.[0wnvu\n",
            "2rw4c\n",
            "—”9…69‘}mg6/{;no9ji},\n",
            "nld:}}7;rr30w(]hb…98éuf[y](hgx1(:7u3”\n",
            "6ênl)/uxi;“b\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We know that we have 60 elements in the vocabulary\n",
        "# so since we are starting with random guesses we expect a loss of\n",
        "# approximately -log(1/60)\n",
        "-torch.log(torch.tensor(1/60))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEgeMdIsCMWp",
        "outputId": "1417c459-9cb4-441e-fd3d-d81cba977bb5"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.0943)"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramModel(len(VOCABULARY))\n",
        "# Move the model to the device\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Create a function to compute the evaluation loss\n",
        "@torch.no_grad()\n",
        "def get_evaluation_loss():\n",
        "  # Define the number of evaluation iterations to have\n",
        "  eval_iters = 20\n",
        "\n",
        "  # Run in evaluation mode\n",
        "  model.eval()\n",
        "  output_losses = {}\n",
        "  for dataset_type in [\"train\", \"test\"]:\n",
        "    loss_tensor = torch.zeros(eval_iters)\n",
        "    for idx_eval in range(eval_iters):\n",
        "      x, y = get_batch(dataset_type)\n",
        "      _, loss = model(x, y)\n",
        "      loss_tensor[idx_eval] = loss.item()\n",
        "    output_losses[dataset_type] = loss_tensor.mean()\n",
        "  model.train()\n",
        "  return output_losses\n",
        "\n",
        "# Now we will train on this model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# We will use a much bigger batch size\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# We want to be able to evaluate the performance of our model\n",
        "# every 100 iterations\n",
        "iteration_eval = 100\n",
        "\n",
        "for steps in range(10000):\n",
        "  if steps % iteration_eval == 0:\n",
        "    # We are in evaluation mode!\n",
        "    loss = get_evaluation_loss()\n",
        "    print(f\"The loss at step {steps} is training loss: '{loss['train']}', \"\\\n",
        "          f\"evaluation loss:'{loss['test']}'\")\n",
        "  x, y = get_batch(\"train\")\n",
        "  _, loss = model(x, y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNHWio4GHWya",
        "outputId": "c3fe59ef-3568-47e8-e634-1baaf287d828"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss at step 0 is training loss: '4.511907577514648', evaluation loss:'4.511220932006836'\n",
            "The loss at step 100 is training loss: '4.364018440246582', evaluation loss:'4.366637229919434'\n",
            "The loss at step 200 is training loss: '4.224114418029785', evaluation loss:'4.222919940948486'\n",
            "The loss at step 300 is training loss: '4.093716144561768', evaluation loss:'4.092387676239014'\n",
            "The loss at step 400 is training loss: '3.963160991668701', evaluation loss:'3.9639041423797607'\n",
            "The loss at step 500 is training loss: '3.8477749824523926', evaluation loss:'3.844214677810669'\n",
            "The loss at step 600 is training loss: '3.7334556579589844', evaluation loss:'3.7337355613708496'\n",
            "The loss at step 700 is training loss: '3.6275527477264404', evaluation loss:'3.629145383834839'\n",
            "The loss at step 800 is training loss: '3.531935453414917', evaluation loss:'3.5333447456359863'\n",
            "The loss at step 900 is training loss: '3.439836025238037', evaluation loss:'3.441093921661377'\n",
            "The loss at step 1000 is training loss: '3.3529906272888184', evaluation loss:'3.3561642169952393'\n",
            "The loss at step 1100 is training loss: '3.272256851196289', evaluation loss:'3.2767536640167236'\n",
            "The loss at step 1200 is training loss: '3.1987662315368652', evaluation loss:'3.2034995555877686'\n",
            "The loss at step 1300 is training loss: '3.128345012664795', evaluation loss:'3.13012957572937'\n",
            "The loss at step 1400 is training loss: '3.0658769607543945', evaluation loss:'3.069307327270508'\n",
            "The loss at step 1500 is training loss: '3.005460023880005', evaluation loss:'3.0072498321533203'\n",
            "The loss at step 1600 is training loss: '2.947984457015991', evaluation loss:'2.9578113555908203'\n",
            "The loss at step 1700 is training loss: '2.8953857421875', evaluation loss:'2.9041285514831543'\n",
            "The loss at step 1800 is training loss: '2.85040020942688', evaluation loss:'2.8570897579193115'\n",
            "The loss at step 1900 is training loss: '2.8069419860839844', evaluation loss:'2.811066150665283'\n",
            "The loss at step 2000 is training loss: '2.767782211303711', evaluation loss:'2.7737436294555664'\n",
            "The loss at step 2100 is training loss: '2.732393264770508', evaluation loss:'2.7376773357391357'\n",
            "The loss at step 2200 is training loss: '2.7017548084259033', evaluation loss:'2.703810214996338'\n",
            "The loss at step 2300 is training loss: '2.665482521057129', evaluation loss:'2.6737890243530273'\n",
            "The loss at step 2400 is training loss: '2.6360247135162354', evaluation loss:'2.6494204998016357'\n",
            "The loss at step 2500 is training loss: '2.613278388977051', evaluation loss:'2.6210474967956543'\n",
            "The loss at step 2600 is training loss: '2.589899778366089', evaluation loss:'2.5986742973327637'\n",
            "The loss at step 2700 is training loss: '2.5614829063415527', evaluation loss:'2.573361873626709'\n",
            "The loss at step 2800 is training loss: '2.5496323108673096', evaluation loss:'2.5591025352478027'\n",
            "The loss at step 2900 is training loss: '2.529183864593506', evaluation loss:'2.5435333251953125'\n",
            "The loss at step 3000 is training loss: '2.5149528980255127', evaluation loss:'2.523616313934326'\n",
            "The loss at step 3100 is training loss: '2.50215482711792', evaluation loss:'2.507551670074463'\n",
            "The loss at step 3200 is training loss: '2.485276699066162', evaluation loss:'2.495868682861328'\n",
            "The loss at step 3300 is training loss: '2.4712977409362793', evaluation loss:'2.4847240447998047'\n",
            "The loss at step 3400 is training loss: '2.460282802581787', evaluation loss:'2.473515272140503'\n",
            "The loss at step 3500 is training loss: '2.4563674926757812', evaluation loss:'2.4644882678985596'\n",
            "The loss at step 3600 is training loss: '2.4498417377471924', evaluation loss:'2.4587271213531494'\n",
            "The loss at step 3700 is training loss: '2.4398350715637207', evaluation loss:'2.4511778354644775'\n",
            "The loss at step 3800 is training loss: '2.438711166381836', evaluation loss:'2.4409055709838867'\n",
            "The loss at step 3900 is training loss: '2.423889636993408', evaluation loss:'2.432598114013672'\n",
            "The loss at step 4000 is training loss: '2.4163296222686768', evaluation loss:'2.426966905593872'\n",
            "The loss at step 4100 is training loss: '2.4143266677856445', evaluation loss:'2.4244251251220703'\n",
            "The loss at step 4200 is training loss: '2.412728786468506', evaluation loss:'2.414463520050049'\n",
            "The loss at step 4300 is training loss: '2.4059040546417236', evaluation loss:'2.4064218997955322'\n",
            "The loss at step 4400 is training loss: '2.402926206588745', evaluation loss:'2.412914991378784'\n",
            "The loss at step 4500 is training loss: '2.397460699081421', evaluation loss:'2.409299373626709'\n",
            "The loss at step 4600 is training loss: '2.39338755607605', evaluation loss:'2.4029176235198975'\n",
            "The loss at step 4700 is training loss: '2.3893446922302246', evaluation loss:'2.4061243534088135'\n",
            "The loss at step 4800 is training loss: '2.3913848400115967', evaluation loss:'2.394973039627075'\n",
            "The loss at step 4900 is training loss: '2.388942003250122', evaluation loss:'2.3925366401672363'\n",
            "The loss at step 5000 is training loss: '2.378875255584717', evaluation loss:'2.388343572616577'\n",
            "The loss at step 5100 is training loss: '2.379503011703491', evaluation loss:'2.394240617752075'\n",
            "The loss at step 5200 is training loss: '2.376774311065674', evaluation loss:'2.381798267364502'\n",
            "The loss at step 5300 is training loss: '2.3754665851593018', evaluation loss:'2.3933701515197754'\n",
            "The loss at step 5400 is training loss: '2.3762269020080566', evaluation loss:'2.379868268966675'\n",
            "The loss at step 5500 is training loss: '2.3773488998413086', evaluation loss:'2.3773462772369385'\n",
            "The loss at step 5600 is training loss: '2.3678176403045654', evaluation loss:'2.3714261054992676'\n",
            "The loss at step 5700 is training loss: '2.3689827919006348', evaluation loss:'2.385728359222412'\n",
            "The loss at step 5800 is training loss: '2.370593786239624', evaluation loss:'2.378248691558838'\n",
            "The loss at step 5900 is training loss: '2.3690760135650635', evaluation loss:'2.380427122116089'\n",
            "The loss at step 6000 is training loss: '2.3656363487243652', evaluation loss:'2.3768553733825684'\n",
            "The loss at step 6100 is training loss: '2.3661370277404785', evaluation loss:'2.3808109760284424'\n",
            "The loss at step 6200 is training loss: '2.3661136627197266', evaluation loss:'2.3755228519439697'\n",
            "The loss at step 6300 is training loss: '2.3644328117370605', evaluation loss:'2.3672146797180176'\n",
            "The loss at step 6400 is training loss: '2.361180305480957', evaluation loss:'2.3727619647979736'\n",
            "The loss at step 6500 is training loss: '2.365154266357422', evaluation loss:'2.3705389499664307'\n",
            "The loss at step 6600 is training loss: '2.3562631607055664', evaluation loss:'2.368346691131592'\n",
            "The loss at step 6700 is training loss: '2.358975410461426', evaluation loss:'2.3644936084747314'\n",
            "The loss at step 6800 is training loss: '2.3614020347595215', evaluation loss:'2.3694941997528076'\n",
            "The loss at step 6900 is training loss: '2.360288143157959', evaluation loss:'2.3725290298461914'\n",
            "The loss at step 7000 is training loss: '2.3546736240386963', evaluation loss:'2.367649555206299'\n",
            "The loss at step 7100 is training loss: '2.361990451812744', evaluation loss:'2.3660614490509033'\n",
            "The loss at step 7200 is training loss: '2.3510706424713135', evaluation loss:'2.3652026653289795'\n",
            "The loss at step 7300 is training loss: '2.357609510421753', evaluation loss:'2.3624720573425293'\n",
            "The loss at step 7400 is training loss: '2.3565008640289307', evaluation loss:'2.3629889488220215'\n",
            "The loss at step 7500 is training loss: '2.3580689430236816', evaluation loss:'2.370966911315918'\n",
            "The loss at step 7600 is training loss: '2.3505375385284424', evaluation loss:'2.3630783557891846'\n",
            "The loss at step 7700 is training loss: '2.353543758392334', evaluation loss:'2.3548035621643066'\n",
            "The loss at step 7800 is training loss: '2.3505051136016846', evaluation loss:'2.361670970916748'\n",
            "The loss at step 7900 is training loss: '2.352569341659546', evaluation loss:'2.361693859100342'\n",
            "The loss at step 8000 is training loss: '2.351585865020752', evaluation loss:'2.3645427227020264'\n",
            "The loss at step 8100 is training loss: '2.358442783355713', evaluation loss:'2.3661866188049316'\n",
            "The loss at step 8200 is training loss: '2.349607467651367', evaluation loss:'2.359898805618286'\n",
            "The loss at step 8300 is training loss: '2.3560895919799805', evaluation loss:'2.3527040481567383'\n",
            "The loss at step 8400 is training loss: '2.3528473377227783', evaluation loss:'2.3619742393493652'\n",
            "The loss at step 8500 is training loss: '2.3492846488952637', evaluation loss:'2.3651998043060303'\n",
            "The loss at step 8600 is training loss: '2.3477559089660645', evaluation loss:'2.3652446269989014'\n",
            "The loss at step 8700 is training loss: '2.3446733951568604', evaluation loss:'2.3602020740509033'\n",
            "The loss at step 8800 is training loss: '2.3463265895843506', evaluation loss:'2.36466383934021'\n",
            "The loss at step 8900 is training loss: '2.348724126815796', evaluation loss:'2.357088327407837'\n",
            "The loss at step 9000 is training loss: '2.3422183990478516', evaluation loss:'2.364849090576172'\n",
            "The loss at step 9100 is training loss: '2.355100631713867', evaluation loss:'2.3545451164245605'\n",
            "The loss at step 9200 is training loss: '2.34794020652771', evaluation loss:'2.360185384750366'\n",
            "The loss at step 9300 is training loss: '2.3470354080200195', evaluation loss:'2.365046739578247'\n",
            "The loss at step 9400 is training loss: '2.3532814979553223', evaluation loss:'2.3648438453674316'\n",
            "The loss at step 9500 is training loss: '2.3511128425598145', evaluation loss:'2.359604597091675'\n",
            "The loss at step 9600 is training loss: '2.3498525619506836', evaluation loss:'2.3562209606170654'\n",
            "The loss at step 9700 is training loss: '2.3458073139190674', evaluation loss:'2.35477352142334'\n",
            "The loss at step 9800 is training loss: '2.3508849143981934', evaluation loss:'2.3592944145202637'\n",
            "The loss at step 9900 is training loss: '2.347689390182495', evaluation loss:'2.357635021209717'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can start generating out of a space\n",
        "input = \" \"\n",
        "tokenized_input = torch.tensor(tokenize(input), dtype=torch.long)\n",
        "# We will do no batching\n",
        "# Input shape will be (1, 1)\n",
        "tokenized_input = tokenized_input.view(1, 1).to(DEVICE)\n",
        "generated_tokens = model.generate(tokenized_input, 100)[0].tolist()\n",
        "generated_text = detokenize(generated_tokens)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xAW29hoN2EY",
        "outputId": "a98e45c9-bc91-471a-9c5c-d168a395006c"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " lve fridin fuspongu’sandis k fo onlyo of mesatwinwngel, hte soinoor be ts watordatonnd m t i sof le \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explaining the Attention mask\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2JoKEMnqVzzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's assume that we are tokenizing using words\n",
        "#\n",
        "# Attention is able to take information from all surounding tokens to predict\n",
        "# the current one\n",
        "#\n",
        "# If the context is \"El Instituto Politecnico _____ es grande\" , attention\n",
        "# should be able to see \"El\" \"Instituto\" \"Politecnico\" but SHOULD NOT be able to\n",
        "# see \"es\" \"grande\" because we are trying to predict \"Nacional\"\n",
        "# (i.e. whatever is in the blank)\n",
        "#\n",
        "# Si nuestra frase ahora es transformada a embeddings de dos elementos, digamos\n",
        "# que:\n",
        "#\n",
        "#. \"El\" - > [1, 2]\n",
        "#. \"Instituto\" -> [34, 21]\n",
        "#. \"Politecnico\" -> [20, 18]\n",
        "#. \"Nacional\" -> [93, 3]\n",
        "#. \"es\" -> [9, 2]\n",
        "#  \"grande\" -> [1, 29]\n",
        "#\n",
        "# En teoria al momento de ver como input a la secuencia\n",
        "# \"El Instituto Politecnico Nacional es grande\" realmente vemos\n",
        "#\n",
        "#.  x = [[1, 2],\n",
        "#        [34,21],\n",
        "#        [20, 18],\n",
        "#        [93, 3],\n",
        "#        [9, 2],\n",
        "#        [1, 29]]\n",
        "#\n",
        "# No obstante, durante el entrenamiento queremos ver solo los tokens anteriores\n",
        "# al que queremos predecir, es decir:\n",
        "#\n",
        "#.  x = [[1, 2],\n",
        "#        [34,21],\n",
        "#        [20, 18],\n",
        "#        [0, 0],\n",
        "#        [0, 0],\n",
        "#        [0, 0]]\n",
        "# Seria util poder crear una \"mascara\" que nos permita elegir cual token usar\n",
        "mask = torch.tril(torch.ones(6, 6)).long()\n",
        "print(mask)\n",
        "x = torch.tensor([[1, 2],\n",
        "                  [34, 21],\n",
        "                  [20, 18],\n",
        "                  [93, 3],\n",
        "                  [9, 2],\n",
        "                  [1, 29]])\n",
        "print(x)\n",
        "result = mask @ x\n",
        "print(\"\\nResult:\")\n",
        "print(\"Look how each row is the sum of the ones above but does not consider\"\\\n",
        "      \"the ones bellow\")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctK_9ECPW7RA",
        "outputId": "b8e0a8b3-39db-41bd-edc0-244c3bfb1f0c"
      },
      "execution_count": 202,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 1, 1]])\n",
            "tensor([[ 1,  2],\n",
            "        [34, 21],\n",
            "        [20, 18],\n",
            "        [93,  3],\n",
            "        [ 9,  2],\n",
            "        [ 1, 29]])\n",
            "\n",
            "Result:\n",
            "Look how each row is the sum of the ones above but does not considerthe ones bellow\n",
            "tensor([[  1,   2],\n",
            "        [ 35,  23],\n",
            "        [ 55,  41],\n",
            "        [148,  44],\n",
            "        [157,  46],\n",
            "        [158,  75]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's imagine now that instead of having a triangular matrix of 1's and 0's\n",
        "# we have a matrix with the attention values that we should use to generate\n",
        "# the next word.\n",
        "\n",
        "mask = torch.tril(torch.rand(6, 6)).float()\n",
        "print(mask)\n",
        "\n",
        "# For the first row, we only want to see the antention values of the first token\n",
        "# and so on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TYVOI5PeAaC",
        "outputId": "d9165636-19c5-4e4c-ff0e-9381acefe954"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.8020, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4224, 0.2088, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.7015, 0.9083, 0.1149, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3915, 0.6450, 0.7232, 0.2031, 0.0000, 0.0000],\n",
            "        [0.9095, 0.7872, 0.5946, 0.8265, 0.9607, 0.0000],\n",
            "        [0.6094, 0.0751, 0.2592, 0.5549, 0.0659, 0.8054]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the GPT Model based on \"Attention is all you need\""
      ],
      "metadata": {
        "id": "gvmdi9l5fYDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DROPOUT = 0.2"
      ],
      "metadata": {
        "id": "ip6w0NbbRMb8"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This class implements a single head self-attention module\n",
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size: int):\n",
        "    super().__init__()\n",
        "\n",
        "    # Keep track of the head_size\n",
        "    self.head_size = head_size\n",
        "\n",
        "    # Create the query, key and value layers\n",
        "    self.query_layer = nn.Linear(EMBEDDING_SIZE, head_size, bias=False)\n",
        "    self.key_layer = nn.Linear(EMBEDDING_SIZE, head_size, bias=False)\n",
        "    self.value_layer = nn.Linear(EMBEDDING_SIZE, head_size, bias=False)\n",
        "\n",
        "    # Create the self-attention mask, remember that it should be as long\n",
        "    # as the number of tokens we will see per iteration, and since attention\n",
        "    # computes scores for each of the other tokens, this is a square matrix\n",
        "    #\n",
        "    # Since this is not a learneable parameter of the model we will store it\n",
        "    # as part of the register buffer that nn.Module already includes\n",
        "    self.register_buffer(\"attention_mask\",\n",
        "                         torch.tril(torch.ones(BLOCK_SIZE, BLOCK_SIZE)))\n",
        "\n",
        "    # Create a dropout layer, this is an optimization not included in the\n",
        "    # original paper\n",
        "    self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # x is shaped (BATCH_SIZE, BLOCK_SIZE, EMBEDDING_SIZE)\n",
        "    # Get the query, key and value vectors, each of these outputs are shaped\n",
        "    # (BATCH_SIZE, BLOCK_SIZE, head_size)\n",
        "    query = self.query_layer(x)\n",
        "    key = self.key_layer(x)\n",
        "    value = self.value_layer(x)\n",
        "\n",
        "    # Compute the attention scores\n",
        "    # Attention will be softmax((QK.T)/sqrt(head_size)) * V\n",
        "    # Now, we need to transpose K because in 3D matrix multiplication if we have\n",
        "    # a matrix A (m, b, l) and a matrix B (m, l, q) the output will be\n",
        "    # a matrix c (m, b, q)\n",
        "    #\n",
        "    # Since query is (BATCH_SIZE, BLOCK_SIZE, head_size) and\n",
        "    # since key is (BATCH_SIZE, BLOCK_SIZE, head_size) we cannot multiply them\n",
        "    # as is, instead we will swap key to be (BATCH_SIZE, head_size, BLOCK_SIZE)\n",
        "    # and thus the result will be\n",
        "    # (BATCH_SIZE, BLOCK_SIZE, BLOCK_SIZE)\n",
        "    QK_transpose = query @ key.transpose(-2, -1) * self.head_size ** -0.5\n",
        "    if torch.any(torch.isnan(QK_transpose)):\n",
        "        print(\"NaNs found in QK_transpose unmasked\")\n",
        "        raise\n",
        "    # Before applying softmax, use the attention mask, that way softmax does not\n",
        "    # consider the scores for tokens it is not supposed to look at\n",
        "    #\n",
        "    # This code uses the attention mask and replaces all values that are 0's\n",
        "    # with -inf, then applies it to QK_transpose\n",
        "    QK_transpose = QK_transpose.masked_fill(self.attention_mask == 0,\n",
        "                                            float(\"-inf\"))\n",
        "    if torch.any(torch.isnan(QK_transpose)):\n",
        "        print(\"NaNs found in QK_transpose masked\")\n",
        "        raise\n",
        "    # Dropout the QK_transpose\n",
        "    #QK_transpose = self.dropout(QK_transpose)\n",
        "    if torch.any(torch.isnan(QK_transpose)):\n",
        "        print(\"NaNs found in QK_transpose dropout\")\n",
        "        raise\n",
        "\n",
        "    # Apply softmax over the last dimention\n",
        "    softmax_QK = F.softmax(QK_transpose, dim=-1)\n",
        "    if torch.any(torch.isnan(softmax_QK)):\n",
        "        print(\"NaNs found in softmax_QK\")\n",
        "        raise\n",
        "    # Softmax_QK is (BATCH_SIZE, BLOCK_SIZE, BLOCK_SIZE)\n",
        "    # Value is (BATCH_SIZE, BLOCK_SIZE, head_size)\n",
        "    # Thus the result will be (BATCH_SIZE, BLOCK_SIZE, head_size)\n",
        "    attention = softmax_QK @ value\n",
        "    if torch.any(torch.isnan(attention)):\n",
        "        print(\"NaNs found in attention\")\n",
        "        raise\n",
        "\n",
        "    return attention\n"
      ],
      "metadata": {
        "id": "U4m_osHbu-1a"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement multihead attention using the attention class we have created\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads: int, head_size: int):\n",
        "    super().__init__()\n",
        "\n",
        "    # Create num_heads of head_size\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    # We will add another linear layer before returning the output\n",
        "    self.linear_layer = nn.Linear(num_heads * head_size, num_heads * head_size)\n",
        "    # Create a dropout layer\n",
        "    self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Run the same input over all the heads in parallel and concatenate all\n",
        "    # of the outputs over the last channel\n",
        "    #\n",
        "    # The output will be (BATCH_SIZE, BLOCK_SIZE, num_heads * head_size)\n",
        "    output = torch.cat([head(x) for head in self.heads], dim=-1)\n",
        "    output = self.linear_layer(output)\n",
        "    output = self.dropout(output)\n",
        "    return output"
      ],
      "metadata": {
        "id": "7LnLJW4LBr-f"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the feed forward at the very end of the transformer\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, embedding_size: int):\n",
        "    super().__init__()\n",
        "\n",
        "    # The original paper, attention is all you need recommends to multiply\n",
        "    # the \"inner dimention\" by 4\n",
        "    self.network = nn.Sequential(\n",
        "      nn.Linear(embedding_size, embedding_size * 4),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(embedding_size * 4, embedding_size),\n",
        "      nn.Dropout(DROPOUT)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.network(x)"
      ],
      "metadata": {
        "id": "jhTlMhWsE9Z4"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a block class, this class has a multihead attention, a\n",
        "# feedforward block and a Normalization (This is what appears as Nx in the\n",
        "# classic image of a transformer)\n",
        "class Block(nn.Module):\n",
        "  def __init__(self,  embedding_size: int, num_heads: int,):\n",
        "    super().__init__()\n",
        "\n",
        "    head_size = embedding_size // num_heads\n",
        "    self.multihead_attention = MultiHeadAttention(num_heads, head_size)\n",
        "    self.feed_forward = FeedForward(embedding_size)\n",
        "    self.layer_norm1 = nn.LayerNorm(embedding_size)\n",
        "    self.layer_norm2 = nn.LayerNorm(embedding_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # We will sum the output to the input to do residual connections\n",
        "    x = x + self.multihead_attention(self.layer_norm1(x))\n",
        "    if torch.any(torch.isnan(x)):\n",
        "        print(\"NaNs found in output of multihead\")\n",
        "        raise\n",
        "    # At this point the input is once again\n",
        "    # (BATCH_SIZE, BLOCK_SIZE, EMBEDDING_SIZE)\n",
        "    x = x + self.feed_forward(self.layer_norm2(x))\n",
        "    if torch.any(torch.isnan(x)):\n",
        "        print(\"NaNs found in feedforward of block\")\n",
        "        raise\n",
        "    return x"
      ],
      "metadata": {
        "id": "fmFw_7MHGD4-"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\"
      ],
      "metadata": {
        "id": "D9RfEBt0GVIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a seed\n",
        "torch.manual_seed(1)\n",
        "\n",
        "# Define the embedding size\n",
        "EMBEDDING_SIZE = 384\n",
        "\n",
        "# Define how many blocks to have\n",
        "NUM_BLOCKS = 6\n",
        "\n",
        "# Create a class for the GPT\n",
        "class GPT(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_heads = 6\n",
        "\n",
        "    # Create an embedding layer that generates embeddings of EMBEDDING_SIZE\n",
        "    self.embedding_layer = nn.Embedding(VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "    # Create a positional embedding layer\n",
        "    # In general we expect at most BLOCK_SIZE tokens per iteration, so for each\n",
        "    # of those positions, we want to generate an embedding (kind of a map\n",
        "    # from position index to an embedding that we can add to the input)\n",
        "    self.positional_embedding_layer = nn.Embedding(BLOCK_SIZE, EMBEDDING_SIZE)\n",
        "    # Create a Block layer composed of multihead attention layer and\n",
        "    # feed forward\n",
        "    # Since we have num_heads and we will concatenate the outputs of each of\n",
        "    # the attention heads, and we expect the dims of the output to be\n",
        "    # (BATCH_SIZE, BLOCK_SIZE, EMBEDDING_SIZE), we need to divide the\n",
        "    # EMBEDDING_SIZE // num_heads to get the head_size\n",
        "    self.blocks = nn.Sequential(\n",
        "        *[Block(EMBEDDING_SIZE, self.num_heads) for _ in range(NUM_BLOCKS)],\n",
        "    )\n",
        "    # Create a normalization layer\n",
        "    self.layer_norm = nn.LayerNorm(EMBEDDING_SIZE)\n",
        "    # Create a linear layer\n",
        "    self.linear_layer = nn.Linear(EMBEDDING_SIZE, VOCAB_SIZE)\n",
        "\n",
        "  def forward(self, x, y = None):\n",
        "    loss = None\n",
        "    # Feed the tokens to the embedding layer\n",
        "    # If the input is (Batch_size, Block_size), the output will be\n",
        "    # (Batch_size, Block_size, Embedding_size) since for each token in the input\n",
        "    # we are creating an embedding\n",
        "    embeddings = self.embedding_layer(x)\n",
        "    if torch.any(torch.isnan(embeddings)):\n",
        "        print(\"NaNs found in embeddings\")\n",
        "        raise\n",
        "    # Get positional embeddings by creating an array from 0 to BLOCK_SIZE - 1\n",
        "    # and creating embeddings out of those\n",
        "    # The output will be (Block_size, Embedding_size)\n",
        "    positional_embeddings = \\\n",
        "      self.positional_embedding_layer(torch.arange(BLOCK_SIZE).to(DEVICE))\n",
        "    if torch.any(torch.isnan(positional_embeddings)):\n",
        "        print(\"NaNs found in possitional embeddings\")\n",
        "        raise\n",
        "    # Sum the embeddings + positional embeddings\n",
        "    # This uses broadcasting so the output will be\n",
        "    # (Batch_size, Block_size, Embedding_size)\n",
        "    embeddings = embeddings + positional_embeddings\n",
        "    # Send the embeddings to the block\n",
        "    output = self.blocks(embeddings)\n",
        "    if torch.any(torch.isnan(output)):\n",
        "        print(\"NaNs found in block\")\n",
        "        raise\n",
        "    # Send the output to the norm layer\n",
        "    output = self.layer_norm(output)\n",
        "    if torch.any(torch.isnan(output)):\n",
        "        print(\"NaNs found in layer norm\")\n",
        "        raise\n",
        "\n",
        "    # The output will be\n",
        "    # (Batch_size, Block_size, Vocab_size)\n",
        "    y_hat = self.linear_layer(output)\n",
        "    if torch.any(torch.isnan(output)):\n",
        "        print(\"NaNs found in linear layer\")\n",
        "        raise\n",
        "\n",
        "    # Compute the loss of the predictions\n",
        "    # y_hat is (Batch_size, Block_size, Vocab_size), and\n",
        "    # y is (Batch_size, Block_size)\n",
        "    #\n",
        "    # In multiclass classification normally the y's are the classes, either\n",
        "    # [1, 2, 9, 2, ... etc]\n",
        "    # And the y_hats are the logits, ie, for each entry in y there is a vector\n",
        "    # of size len(classes) where index of the element with the biggest value is\n",
        "    #. expecting to match y[idx], example:\n",
        "    #\n",
        "    # Suppose we have 5 classes\n",
        "    # y[0] = 1\n",
        "    # y_hat[0] = [0, 0.8, 0.05, 0.05, 0]\n",
        "    #\n",
        "    # This means that we need y to be an array of the expected classes rather\n",
        "    # than a matrix i.e.\n",
        "    #\n",
        "    #.   y = [[34,  1, 53, 35],\n",
        "    #         [0, 56, 17, 39]]\n",
        "    #\n",
        "    # We really want y to be the whole sequence:\n",
        "    #.   y = [34,  1, 53, 35, 0, 56, 17, 39]\n",
        "    if y is not None:\n",
        "      y = y.view(BATCH_SIZE * BLOCK_SIZE)\n",
        "    # Similarly since the NN is trying to predict EVERY word in the sequence\n",
        "    # and instead of tokens we get embeddings, then we want y_hat to be a matrix\n",
        "    # of size (batch_size * block_size, vocab_size), i.e\n",
        "    #\n",
        "    #.  Assume that 'a', 'b', ...'z' are embeddings predicted by the NN of size\n",
        "    #.  vocab_size each... i.e. 'a' = [003, 53465, 5352342, ..., 4354]\n",
        "    #\n",
        "    #.   y_hat = [[a, b, c, d],\n",
        "    #             [b, k, d, c]]\n",
        "    #\n",
        "    #.   y_hat = [a, b, c, d, b, k, d, c]\n",
        "    #\n",
        "    # In this way, similarly to the inicial example where binary cross entropy\n",
        "    # compared an scalar to a vector, we can do:\n",
        "    #\n",
        "    #       y = [34, 1, 53, 35, 0, 56, 17, 39]\n",
        "    #                        vs\n",
        "    #   y_hat = [a,  b,  c,  d, b,  k,  d,  c]\n",
        "    #\n",
        "    #. And hope that 'a' should be the embedding for 34\n",
        "    if y is not None:\n",
        "      batch_size, block_size, vocab_size = y_hat.shape\n",
        "      y_hat = y_hat.view(batch_size * block_size, vocab_size)\n",
        "      loss = F.cross_entropy(y_hat, y)\n",
        "\n",
        "      # Restore y_hat to return a consistent size\n",
        "      y_hat = y_hat.view(batch_size, block_size, vocab_size)\n",
        "    return y_hat, loss\n",
        "\n",
        "  def generate(self, x, max_new_tokens):\n",
        "    # x has the shape (BATCH_SIZE, BLOCK_SIZE)\n",
        "    for _ in range(max_new_tokens):\n",
        "      # Ensure that our context is never bigger than BLOCK_SIZE\n",
        "      x_context = x[:, -BLOCK_SIZE:]\n",
        "\n",
        "      # Get the predictions\n",
        "      # y_hat is shaped (BATCH_SIZE, BLOCK_SIZE, vocab_size) because we did not\n",
        "      # provided a true labe to the forward function\n",
        "      y_hat, _ = self(x_context)\n",
        "\n",
        "      # Get only the last generated embedding for each element in the batch\n",
        "      # This will become a (BATCH_SIZE, vocab_size)\n",
        "      last_embeddings = y_hat[:, -1, :]\n",
        "\n",
        "      # Run it over softmax\n",
        "      # (BATCH_SIZE, vocab_size)\n",
        "      last_embeddings_scores = F.softmax(last_embeddings, dim=-1)\n",
        "\n",
        "      # Get the index of the biggest score\n",
        "      # (BATCH_SIZE, 1)\n",
        "      high_score_indexes = torch.multinomial(last_embeddings_scores,\n",
        "                                             num_samples=1)\n",
        "      # Concatenate the predicted scores to the input for the next generation\n",
        "      # (BATCH_SIZE, BLOCK_SIZE + 1)\n",
        "      x = torch.cat((x, high_score_indexes), dim=1)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "F4LBaP23fdMC"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT()\n",
        "# Move the model to the device\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Create a function to compute the evaluation loss\n",
        "@torch.no_grad()\n",
        "def get_evaluation_loss():\n",
        "  # Define the number of evaluation iterations to have\n",
        "  eval_iters = 200\n",
        "\n",
        "  # Run in evaluation mode\n",
        "  model.eval()\n",
        "  output_losses = {}\n",
        "  for dataset_type in [\"train\", \"test\"]:\n",
        "    loss_tensor = torch.zeros(eval_iters)\n",
        "    for idx_eval in range(eval_iters):\n",
        "      x, y = get_batch(dataset_type)\n",
        "      _, loss = model(x, y)\n",
        "      loss_tensor[idx_eval] = loss.item()\n",
        "    output_losses[dataset_type] = loss_tensor.mean()\n",
        "  model.train()\n",
        "  return output_losses\n",
        "\n",
        "# Now we will train on this model\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "\n",
        "\n",
        "# We want to be able to evaluate the performance of our model\n",
        "# every 100 iterations\n",
        "iteration_eval = 100\n",
        "\n",
        "for steps in range(5000):\n",
        "  if steps % iteration_eval == 0:\n",
        "    # We are in evaluation mode!\n",
        "    loss = get_evaluation_loss()\n",
        "    print(f\"The loss at step {steps} is training loss: '{loss['train']}', \"\\\n",
        "          f\"evaluation loss:'{loss['test']}'\")\n",
        "  x, y = get_batch(\"train\")\n",
        "  _, loss = model(x, y)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhN_nzBsgdo2",
        "outputId": "763ade0e-63f9-4a4c-baea-c133110fe3a4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The loss at step 0 is training loss: '4.236394882202148', evaluation loss:'4.23798131942749'\n",
            "The loss at step 100 is training loss: '2.3636019229888916', evaluation loss:'2.3738908767700195'\n",
            "The loss at step 200 is training loss: '2.323028087615967', evaluation loss:'2.3316962718963623'\n",
            "The loss at step 300 is training loss: '2.2493088245391846', evaluation loss:'2.2597362995147705'\n",
            "The loss at step 400 is training loss: '2.091914176940918', evaluation loss:'2.105107069015503'\n",
            "The loss at step 500 is training loss: '1.9639816284179688', evaluation loss:'1.978714108467102'\n",
            "The loss at step 600 is training loss: '1.8738656044006348', evaluation loss:'1.8928455114364624'\n",
            "The loss at step 700 is training loss: '1.7828125953674316', evaluation loss:'1.8044450283050537'\n",
            "The loss at step 800 is training loss: '1.7072608470916748', evaluation loss:'1.7311569452285767'\n",
            "The loss at step 900 is training loss: '1.6510440111160278', evaluation loss:'1.6809784173965454'\n",
            "The loss at step 1000 is training loss: '1.6054905652999878', evaluation loss:'1.6338993310928345'\n",
            "The loss at step 1100 is training loss: '1.5655947923660278', evaluation loss:'1.593841552734375'\n",
            "The loss at step 1200 is training loss: '1.5268797874450684', evaluation loss:'1.5631608963012695'\n",
            "The loss at step 1300 is training loss: '1.4941216707229614', evaluation loss:'1.5339056253433228'\n",
            "The loss at step 1400 is training loss: '1.4709569215774536', evaluation loss:'1.5072157382965088'\n",
            "The loss at step 1500 is training loss: '1.4460537433624268', evaluation loss:'1.4878648519515991'\n",
            "The loss at step 1600 is training loss: '1.4316551685333252', evaluation loss:'1.47442626953125'\n",
            "The loss at step 1700 is training loss: '1.4103273153305054', evaluation loss:'1.4549237489700317'\n",
            "The loss at step 1800 is training loss: '1.3972245454788208', evaluation loss:'1.4401803016662598'\n",
            "The loss at step 1900 is training loss: '1.378797173500061', evaluation loss:'1.421686053276062'\n",
            "The loss at step 2000 is training loss: '1.3662636280059814', evaluation loss:'1.4136172533035278'\n",
            "The loss at step 2100 is training loss: '1.356105089187622', evaluation loss:'1.4037190675735474'\n",
            "The loss at step 2200 is training loss: '1.3429760932922363', evaluation loss:'1.386655569076538'\n",
            "The loss at step 2300 is training loss: '1.333553433418274', evaluation loss:'1.378130316734314'\n",
            "The loss at step 2400 is training loss: '1.3261303901672363', evaluation loss:'1.3680235147476196'\n",
            "The loss at step 2500 is training loss: '1.3131107091903687', evaluation loss:'1.360967755317688'\n",
            "The loss at step 2600 is training loss: '1.3047889471054077', evaluation loss:'1.3514704704284668'\n",
            "The loss at step 2700 is training loss: '1.2952381372451782', evaluation loss:'1.3453096151351929'\n",
            "The loss at step 2800 is training loss: '1.285042405128479', evaluation loss:'1.336753487586975'\n",
            "The loss at step 2900 is training loss: '1.2783539295196533', evaluation loss:'1.3308786153793335'\n",
            "The loss at step 3000 is training loss: '1.276931881904602', evaluation loss:'1.3226720094680786'\n",
            "The loss at step 3100 is training loss: '1.268206000328064', evaluation loss:'1.319380283355713'\n",
            "The loss at step 3200 is training loss: '1.2564047574996948', evaluation loss:'1.3084461688995361'\n",
            "The loss at step 3300 is training loss: '1.2525885105133057', evaluation loss:'1.3092904090881348'\n",
            "The loss at step 3400 is training loss: '1.2452521324157715', evaluation loss:'1.2979501485824585'\n",
            "The loss at step 3500 is training loss: '1.2418493032455444', evaluation loss:'1.2973222732543945'\n",
            "The loss at step 3600 is training loss: '1.2381561994552612', evaluation loss:'1.2886667251586914'\n",
            "The loss at step 3700 is training loss: '1.2323113679885864', evaluation loss:'1.2913360595703125'\n",
            "The loss at step 3800 is training loss: '1.2289514541625977', evaluation loss:'1.2829257249832153'\n",
            "The loss at step 3900 is training loss: '1.2225674390792847', evaluation loss:'1.2793433666229248'\n",
            "The loss at step 4000 is training loss: '1.2174683809280396', evaluation loss:'1.276961326599121'\n",
            "The loss at step 4100 is training loss: '1.2150208950042725', evaluation loss:'1.2754472494125366'\n",
            "The loss at step 4200 is training loss: '1.2064342498779297', evaluation loss:'1.2639963626861572'\n",
            "The loss at step 4300 is training loss: '1.204915165901184', evaluation loss:'1.2605621814727783'\n",
            "The loss at step 4400 is training loss: '1.2015821933746338', evaluation loss:'1.2617071866989136'\n",
            "The loss at step 4500 is training loss: '1.1974916458129883', evaluation loss:'1.2545281648635864'\n",
            "The loss at step 4600 is training loss: '1.1918907165527344', evaluation loss:'1.2537764310836792'\n",
            "The loss at step 4700 is training loss: '1.1909427642822266', evaluation loss:'1.2489702701568604'\n",
            "The loss at step 4800 is training loss: '1.186752438545227', evaluation loss:'1.2446728944778442'\n",
            "The loss at step 4900 is training loss: '1.1844998598098755', evaluation loss:'1.2419917583465576'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate text\n",
        "# We can start generating out of a space\n",
        "\n",
        "tokenized_input = torch.tensor([[39] * BLOCK_SIZE], dtype=torch.long).to(DEVICE)\n",
        "# We will do no batching\n",
        "# Input shape will be (1, 1)\n",
        "generated_tokens = model.generate(tokenized_input, 5000)[0].tolist()\n",
        "generated_text = detokenize(generated_tokens)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmmcF8TD_eNL",
        "outputId": "b8322d12-0052-4d5a-a3ff-7289b6435916"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                                                                                                                                                                                                                                tyrion, of scraved up and about me and west nother’s house of doubtless, even torged crownsignging. “leallocks and fand win traps, and that’s song?”\n",
            "\n",
            "“no jeaght of cell, play where stannis aid. the trees of my grain and is it friends. then this men said abefore, and i have load may be along for foight not less so cressen of balon,” pows wenster, his sword hang. hot peep the fast. she large that the battle of the survivor and darkness to lamp, glory, and then never sat “be afore the auttach of sweet and cats, a few horse past.”\n",
            "\n",
            "“joffrey the king others didn’t twenty back wife, another,” more in the loomy roof. “the own hilt’s so leborn men. youthing, my lady grand without.”\n",
            "\n",
            "“did you ride?” wild ser raymun took her, they pyried and dany pushed apologs openly. morrid licked beside his praying.\n",
            "\n",
            "“my lady…” he said. “she was coming firedly,” whispered.\n",
            "\n",
            "ser decla had a princed.\n",
            "\n",
            "“driwink hodor’s threat in usuadded while wominks. his kingly sense if ser ser hors too much. the wildlings were lord balannistanched easant.”\n",
            "\n",
            "“besed you want there, it’s sweet the bastard swirling who loved off you, some more can point to great was it and tyrion realmember huntered tinguins for all years to me copplain.”\n",
            "\n",
            "maester luwin seemed the world on it. “at the summer knight with maps for batin cheek.”\n",
            "\n",
            "“i awmanned, i give rickon’s full and hard to send of ward?”\n",
            "\n",
            "“the blood was good in storipped have the use many fair and all warman?” tyrion said. “you preach a litt of myself, to go joing the stroken, the with air.” she saw, luck and him old stood. “speak that’s four?” her sharp like a smush again tunic. the vospered of the steps. “who have the blood iron throbed.”\n",
            "\n",
            "“see?” sansa said the queen’s well. “get lord stannis and slended closer on while she still. gtand does the what know…\n",
            "\n",
            "he feared as one, your bloody holdwis damn, and near happened, the fat. stew catelyn’s studering back. as he was done, so whated as she was let her father’s? “you might can’t tend?”\n",
            "\n",
            "“get uncustop shold envoy bloody to run be sweep lannispord tells whose my heir tongue more,” jory arrowed. “i am. i could hear incless thoses. they called your scends knownse, your graced massive tyry of their between all yourself and said upseen, truthird their westesterness. tyrion held him a sound, and we send her shedchells not, before.” fight easy. “i take our ingent mine me officuser thade your grace.” he closed pink and fought it a was fainciest at over her regained.\n",
            "\n",
            "“i don’t want to. you, is it it!”\n",
            "\n",
            "“it is! is not,” safe admided the fireles. the marriager was witding astapons as did well, and mulad could see the power of the garlen oght had ever keep so hard,” said plump sounded, ““i have a best clothing and choiced to when pypt me herself besideside costered.” he pointed between the moment swords of surted lysa swifted. “there is she whore,” the birch off it seemed her back off astap, and the farrow hould not could not be jewel come in the ttrest. and there put he heard send a crestled. “you’d given it only a rest people, see so geldising.”\n",
            "\n",
            "kevan that, he smiling. hell me she heard, and steppedding over her other to her ragge who my wind spears, out tides to bow than apposted to catch her glance?”\n",
            "\n",
            "robb stood his chest was to him. jason portsed a maid on the of forests of the axe weight. seemed a horst followed pyp of his nate. handrik. “the imagine,” he melisandred said and put it up out to strike suddenly.\n",
            "\n",
            "“had i done awaid? whatever, some nit? and why the finally salt tire. girls and me to each. and fishness, strying the other stafford.”\n",
            "\n",
            "a trut small knighted even the slatter. “bad his left and the treator on house.”\n",
            "\n",
            "“his father’s no song.”\n",
            "\n",
            "“nor good you ho?” theon said waited it only betward. “if me about i reaches meady sire fir to be, my lord day’s pressed, and how he callsmeved out, the three cut. i as ask to kind it. i’d see had to dead us to castle well no cold…\n",
            "\n",
            "“is horus visit?”\n",
            "\n",
            "maester, in the hunglasp and darkness, the one nice untestily. so the knight in the andal, his mouth cregs eldest sigcle, in a, and splash or wines, speaking balon and speaked with green leather triumped blook at castle, and soouth cut bowled linking against the eagles, but the name seem above what was. a churleyed blowing guarding over their way of four chainmark that priding elmed that boy with the judge of far man, calling the mother’s fire.” catelyn thought she found the sellsword urprise, and and the new insisted the safe.\n",
            "\n",
            "the raider was cold and foster knot on the trees, the gods he will sun as broth all of flown disfeat, lost its the teached, roble and filled his head… human even he never brushed. full clumsy, cludness, oyell, though and jhymo princed women when brother was taken on winter painned. jon was lemy, in the fighting the night pardoned in mie. “who didn’t i believe it do,” he armn’s brother said, “\n",
            "\n",
            "“dane. it is some trotts, too bark of him, and licked to friends than the usunderlitt of her seunds from hundred, he wasn’t a stark messwept what nev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"This model has the following number of parameters\")\n",
        "print(sum([param.numel() for param in model.parameters()]) /1e6, \"million\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcXJjrhTmbxf",
        "outputId": "a89c9e17-703a-4bd9-960f-744e6f12e5f7"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This model has the following number of parameters\n",
            "10.785084 million\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ppSKW17-nE4F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}